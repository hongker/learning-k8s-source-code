* [1\. 背景](#1-背景)
* [2\. 如何理解network namespaces](#2-如何理解network-namespaces)
* [3\. 不同namespaces之间是如何通信的](#3-不同namespaces之间是如何通信的)
  * [3\.1 创建network namespace](#31-创建network-namespace)
  * [3\.2 两个networknamespaces之间的通信](#32-两个networknamespaces之间的通信)
* [4\. 多个namespaces之间的通信](#4-多个namespaces之间的通信)
  * [4\.1 创建3个namespaces](#41-创建3个namespaces)
  * [4\.2 创建bridge](#42-创建bridge)
  * [4\.3 创建 veth pair](#43-创建-veth-pair)
  * [4\.4 将 veth pair 的一头挂到 namespace 中，一头挂到 bridge 上，并设 IP 地址](#44-将-veth-pair-的一头挂到-namespace-中一头挂到-bridge-上并设-ip-地址)
  * [4\.5 验证多Namespaces互通](#45-验证多namespaces互通)
* [5\. 补充](#5-补充)
  * [5\.1 如何查看容器内和 宿主的 veth pair对](#51-如何查看容器内和-宿主的-veth-pair对)

### 1. 背景

上文提到的docker 4中网络模式，核心就是network namespaces的不同，比如可以共享宿主的network(hostnetwork)。容器网络模式的核心就是：

通过network namespaces隔离了各个容器，然后通过设置veth pari, bride等虚拟网络设备来达到   容器networks 与宿主hostnetwork的通信。最终是通过宿主的物理网卡进行了传输。

本节就是说明一些docker 网络到底是如何实现的。

<br>

### 2. 如何理解network namespaces

摘抄自：容器实战高手课-极客时间

对于 Network Namespace，我们从字面上去理解的话，可以知道它是在一台 Linux 节点上对网络的隔离，不过它具体到底隔离了哪部分的网络资源呢？

我们还是先来看看操作手册，在Linux Programmer’s Manual里对 [Network Namespace](https://man7.org/linux/man-pages/man7/network_namespaces.7.html) 有一个段简短的描述，在里面就列出了最主要的几部分资源，它们都是通过 Network Namespace 隔离的。

我把这些资源给你做了一个梳理：

* 第一种，网络设备，这里指的是 lo，eth0 等网络设备。你可以通过 ip link命令看到它们。

* 第二种是 IPv4 和 IPv6 协议栈。从这里我们可以知道，IP 层以及上面的 TCP 和 UDP 协议栈也是每个 Namespace 独立工作的。所以 IP、TCP、UDP 的很多协议，它们的相关参数也是每个 Namespace 独立的，这些参数大多数都在 /proc/sys/net/ 目录下面，同时也包括了 TCP 和 UDP 的 port 资源。
* 第三种，IP 路由表，这个资源也是比较好理解的，你可以在不同的 Network Namespace 运行 ip route 命令，就能看到不同的路由表了。
* 第四种是防火墙规则，其实这里说的就是 iptables 规则了，每个 Namespace 里都可以独立配置 iptables 规则。
* 最后一种是网络的状态信息，这些信息你可以从 /proc/net 和 /sys/class/net 里得到，这里的状态基本上包括了前面 4 种资源的的状态信息。

<br>

再结合前面笔记，关于协议栈部分的介绍。network namesapces 隔离了 网络设备，网络参数，协议栈配置等等信息。这样network namespaces里面的包发送自然会受到限制，从而达到了隔离的作用。

### 3. 不同namespaces之间是如何通信的

namespaces的隔离很简单：一个新的namespace啥都没有，只有一个lo，只能访问自己

通信就需要网络设备了，这里介绍一下docker常用的veth pair对和  bridge。

#### 3.1 创建network namespace

参考：https://www.cnblogs.com/bakari/p/10443484.html

（1）创建namespaces

（2）每个 namespace 在创建的时候会自动创建一个回环接口 lo ，默认不启用，可以通过 ip link set lo up 启用

```
// 1.创建namespaces
root@k8s-node:~#ip netns add netns1

root@k8s-node:~# ip netns ls
netns1

root@k8s-node:~# ip netns  exec netns1 ip addr
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00

// 2.每个 namespace 在创建的时候会自动创建一个回环接口 lo ，默认不启用，可以通过 ip link set lo up 启用。
root@k8s-node:~# ip netns  exec netns1 bash
root@k8s-node:~# ping www.baidu.com
ping: www.baidu.com: Temporary failure in name resolution
root@k8s-node:~# ping 127.0.0.1
connect: Network is unreachable
root@k8s-node:~# ip link set lo up
root@k8s-node:~# ping 127.0.0.1
PING 127.0.0.1 (127.0.0.1) 56(84) bytes of data.
64 bytes from 127.0.0.1: icmp_seq=1 ttl=64 time=0.021 ms
64 bytes from 127.0.0.1: icmp_seq=2 ttl=64 time=0.016 ms
64 bytes from 127.0.0.1: icmp_seq=3 ttl=64 time=0.018 ms
64 bytes from 127.0.0.1: icmp_seq=4 ttl=64 time=0.027 ms
^Z
[1]+  Stopped                 ping 127.0.0.1
```

#### 3.2 两个networknamespaces之间的通信

（1）再创建一个namespaces

```
root@k8s-node:~#  ip netns add netns0
root@k8s-node:~# ip netns ls
netns0
netns1
```

（2）生成一堆veth pair

```
root@k8s-node:~# ip link add type veth

root@k8s-node:~# ip link
42: veth0@veth1: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 62:bb:3c:a3:ac:31 brd ff:ff:ff:ff:ff:ff
43: veth1@veth0: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 32:9d:76:89:b4:7a brd ff:ff:ff:ff:ff:ff
```

（3）给 veth pair 配上 ip 地址

```
// 进入netns0将veth0设备启动
root@k8s-node:~# ip netns exec netns0 ip link set veth0 up

// 查看已经开启了，UP
root@k8s-node:~# ip netns exec netns0 ip addr
1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
42: veth0@if43: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc noqueue state LOWERLAYERDOWN group default qlen 1000
    link/ether 62:bb:3c:a3:ac:31 brd ff:ff:ff:ff:ff:ff link-netns netns1
    
// 进入netns1将veth1设备启动
root@k8s-node:~# ip netns exec netns1 ip link set veth1 up
root@k8s-node:~# ip netns exec netns1 ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
43: veth1@if42: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default qlen 1000
    link/ether 32:9d:76:89:b4:7a brd ff:ff:ff:ff:ff:ff link-netns netns0
    inet6 fe80::309d:76ff:fe89:b47a/64 scope link 
       valid_lft forever preferred_lft forever
```

(4) 给veth pair网卡配置Ip

```
veth0 对应 netns0，对应Ip段10.1.1.1/24
root@k8s-node:~#  ip netns exec netns0 ip addr add 10.1.1.1/24 dev veth0

veth1 对应 netns1，对应Ip段10.1.1.2/24
root@k8s-node:~# ip netns exec netns1 ip addr add 10.1.1.2/24 dev veth1

netns0 ping veth1对应的ip段可通
root@k8s-node:~#  ip netns exec netns0 ping 10.1.1.2
PING 10.1.1.2 (10.1.1.2) 56(84) bytes of data.
64 bytes from 10.1.1.2: icmp_seq=1 ttl=64 time=0.038 ms
64 bytes from 10.1.1.2: icmp_seq=2 ttl=64 time=0.022 ms
64 bytes from 10.1.1.2: icmp_seq=3 ttl=64 time=0.023 ms
^Z
[1]+  Stopped                 ip netns exec netns0 ping 10.1.1.2
```

### 4. 多个namespaces之间的通信

参考：https://www.cnblogs.com/bakari/p/10443484.html

2 个 namespace 之间通信可以借助 `veth pair` ，多个 namespace 之间的通信则可以使用 bridge 来转接，不然每两个 namespace 都去配 `veth pair` 将会是一件麻烦的事。下面就看看如何使用 bridge 来转接。

拓扑图如下：

![image-20220327163813484](../images/docker-net-1.png)

#### 4.1 创建3个namespaces

```
root@k8s-node:~# ip netns add net0
root@k8s-node:~# ip netns add net1
root@k8s-node:~# ip netns add net2
```

#### 4.2 创建bridge

```
root@k8s-node:~# ip link add br0 type bridge
// 开启这个网络设备
root@k8s-node:~# ip link set dev br0 up

root@k8s-node:~# ip link 

57: br0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/ether 5e:3c:aa:99:dc:09 brd ff:ff:ff:ff:ff:ff
```

####  4.3 创建 veth pair

```
//（1）创建 3 个 veth pair
# ip link add type veth
# ip link add type veth
# ip link add type veth


veth是递增的，所以这三对是 23， 45， 67这三对
root@k8s-node:~# ip link
44: veth2@veth3: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 4e:11:eb:21:3a:16 brd ff:ff:ff:ff:ff:ff
45: veth3@veth2: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 1a:74:4a:dd:98:2d brd ff:ff:ff:ff:ff:ff
46: veth4@veth5: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 1e:f5:74:3f:ae:00 brd ff:ff:ff:ff:ff:ff
47: veth5@veth4: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 36:a6:e6:d8:49:53 brd ff:ff:ff:ff:ff:ff
48: veth8625ade0@if3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT group default 
    link/ether f6:57:1a:a5:65:f7 brd ff:ff:ff:ff:ff:ff link-netnsid 0
55: veth6@veth7: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether 5a:37:5e:74:01:0f brd ff:ff:ff:ff:ff:ff
56: veth7@veth6: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
    link/ether fe:9c:e3:75:6c:23 brd ff:ff:ff:ff:ff:ff
```

#### 4.4 将 veth pair 的一头挂到 namespace 中，一头挂到 bridge 上，并设 IP 地址

```
// 配置第一个ns 和 bridge
// 将veth2 挂到 net0 这个命名空间下
root@k8s-node:~# ip link set dev veth2 netns net0

// 将namespaces ip link看到的veth2 改名为eth0
root@k8s-node:~# ip netns exec net0 ip link set dev veth2 name eth0

// 设置ip 10.0.1.1/24
root@k8s-node:~# ip netns exec net0 ip addr add 10.0.1.1/24 dev eth0

// 开启网络设备eth0，其实就是veth2
root@k8s-node:~# ip netns exec net0 ip link set dev eth0 up

// 将veth3 挂着bro网桥上
root@k8s-node:~# ip link set dev veth3 master br0

// 开启bridge的网络设备 veth3
root@k8s-node:~# ip link set dev veth3 up



// 配置第 2 个 net1
# ip link set dev veth4 netns net1
# ip netns exec net1 ip link set dev veth4 name eth0
# ip netns exec net1 ip addr add 10.0.1.2/24 dev eth0
# ip netns exec net1 ip link set dev eth0 up
#
# ip link set dev veth5 master br0
# ip link set dev veth5 up



// 配置第 3 个 net2 (这里我配错了一个，重新又生成了1对，所以是veth0,veth1)
# ip link set dev veth0 netns net2
# ip netns exec net2 ip link set dev veth0 name eth0
# ip netns exec net2 ip addr add 10.0.1.2/24 dev eth0
# ip netns exec net2 ip link set dev eth0 up
#
# ip link set dev veth1 master br0
# ip link set dev veth1 up
```

#### 4.5 验证多Namespaces互通

这样之后，竟然通不了，经查阅 [参见](https://segmentfault.com/q/1010000010011053/a-1020000010025650) ，是因为

> 原因是因为系统为bridge开启了iptables功能，导致所有经过br0的数据包都要受iptables里面规则的限制，而docker为了安全性（我的系统安装了 docker），将iptables里面filter表的FORWARD链的默认策略设置成了drop，于是所有不符合docker规则的数据包都不会被forward，导致你这种情况ping不通。
>
> 解决办法有两个，二选一：
>
> 1. 关闭系统bridge的iptables功能，这样数据包转发就不受iptables影响了：echo 0 > /proc/sys/net/bridge/bridge-nf-call-iptables
> 2. 为br0添加一条iptables规则，让经过br0的包能被forward：iptables -A FORWARD -i br0 -j ACCEPT
>
> 第一种方法不确定会不会影响docker，建议用第二种方法。

我采用以下方法解决：

`Copyiptables -A FORWARD -i br0 -j ACCEPT`



```
root@k8s-node:~# ip netns exec net0 ping -c 2 10.0.1.2
PING 10.0.1.2 (10.0.1.2) 56(84) bytes of data.


^X
^Z
[2]+  Stopped                 ip netns exec net0 ping -c 2 10.0.1.2


root@k8s-node:~# iptables -A FORWARD -i br0 -j ACCEPT
root@k8s-node:~# 


root@k8s-node:~# ip netns exec net0 ping -c 2 10.0.1.2
PING 10.0.1.2 (10.0.1.2) 56(84) bytes of data.
64 bytes from 10.0.1.2: icmp_seq=1 ttl=64 time=0.061 ms
64 bytes from 10.0.1.2: icmp_seq=2 ttl=64 time=0.036 ms

--- 10.0.1.2 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 12ms
rtt min/avg/max/mdev = 0.036/0.048/0.061/0.014 ms


```

<br>

### 5. 补充

#### 5.1 如何查看容器内和 宿主的 veth pair对

可以参考：https://blog.csdn.net/u011563903/article/details/88593251

